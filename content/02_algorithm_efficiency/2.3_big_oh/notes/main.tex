\documentclass{aldast}

\usepackage{arydshln}
\usepackage{subfig}

\documentType{Lecture Notes}
\documentNumber{2.3}
\title{Orders of Growth}
\author{F. Chauvel}


\begin{document}

\maketitle

\begin{abstract}
  We look at how to abstract away the details of the RAM model to obtain
  more efficiency models, using asymptotic analysis. We discuss the
  pros and cons of this approach.
\end{abstract}


\section*{Introduction}
We have now seen how to model the efficiency of an algorithm, using
functions that map the size of the given inputs to either the number
of instructions executed or the number of memory cell used. This let
us predict how much resources a calculation may requires, but in
practice, what we need is to compare the efficiency of alternative
algorithms. This is the purpose of this lecture.

The principle is simple: We take the efficiency models and we put them
into broad categories, called \emph{growth orders}, which capture of
fast the resource consumption grows with the input size. For example,
we distinguish, between logarithmic growths, linear growths, quadratic
growths, etc.

In fine, there is often no best algorithm. Some may consume more
memory but be faster whereas other may be slower may consume less
memory. It is often, if not always, a trade-off. Besides, it might
very well be that some algorithm a better ``best case'' but a worst
case, etc. Comparing efficiency is complicated.

I divided this lecture into two parts. First we will see two
alternative algorithms to compute the average of a sequence of
number. We will see that their efficiency depends on the underlying
computing model. Then, we will review asymptotic analysis, a tool that
will help us identify to which growth order a given model belong.


\section{Comparing Efficiencies}
\label{sec:generality}

Consider for example the two algorithms shown on
Figure~\ref{alg:averages}. Both compute the average of the given
sequence, but using different approaches. The first one
(Figure~\ref{alg:average}) loops through the given values, adding them
up, and finally divides this total by the number of values.  By
contrast, the ``online'' average (see Figure~\ref{alg:online-average})
starts with the average initially set to 0 and then adjusts this
average for every given number. Which one is the fastest? Which one
consume the least memory?

\begin{figure}[htbp]
  \marginnote{
    \includestandalone[width=\marginparwidth]{images/averages.tikz}
    \captionof{figure}{Efficiency models of the classical and online
      average algorithms.}
    \label{plot:averages}
  }[-0.75cm]
  \centering
  \subfloat[Classical average] {
    \scalebox{0.85}{
      \begin{minipage}{7cm}
        \begin{algorithm}[H]
          \KwIn{$\mathbf{s} = (s_1, s_2, \ldots, s_n) \in \mathbb{N}^n$}
          \KwOut{$\mu = \frac{1}{n} \sum_{i=1}^{n} s_i$}
          $i \gets 1$\;
          $sum \gets 0$\;
          \While{$i \leq n$}{
            $sum \gets sum + s_i$\;
            $i \gets i+1$\;
          }
          \Return{$sum / n$}\;
        \end{algorithm}
      \end{minipage}}
    \label{alg:average}
  }
  \subfloat[Online average]
  {
    \scalebox{0.85}{
      \begin{minipage}{7cm}
        \begin{algorithm}[H]
          \KwIn{$\mathbf{s} = (s_1, s_2, \ldots, s_n) \in \mathbb{N}^n$}
          \KwOut{$\mu = \frac{1}{n} \sum_{i=1}^{n} s_i$}
          $i \gets 1$\;
          $\mu \gets 0$\;
          \While{$i \leq n$}{
            $\mu \gets \mu + \frac{s_i-\mu}{i+1}$\;
            $i \gets i+1$\;
          }
          \Return{$\mu$}\;
        \end{algorithm}
      \end{minipage}}
    \label{alg:online-average}
  }
  \caption{Two alternative algorithms that compute the average of a
    sequence of numbers}
  \label{alg:averages}
\end{figure}

Algorithm efficiency helps us here, because we can compare their
efficiency model. Consider the runtime for instance. We saw in the
previous lecture that the ``classical'' average requires $5n+4$
instructions, whereas we can now estimate that the ``online'' average
would require $8n + 3$ instructions. These models are simple enough
(there is no best, worst or average case) and we see on
Figure~\ref{plot:averages} and from the formulae we obtain that the
classical algorithm is always faster. For example, the classical
average would take 104 operations for a sequence of 20 numbers,
whereas the online algorithm would need 163.

\begin{takeaway}
  We compare algorithms' efficiency by comparing their efficiency
  models. The comparison is seldom straightforward as best, worst and
  average case comparisons may not agree.
\end{takeaway}

\paragraph{Precision vs. Generality}

The challenge is that such efficiency models are difficult to get on
real-life algorithms. The mathematics quickly become
non-trivial\sidenote{Some algorithms, have no proven efficiency
  yet}. Ideally, we would like to reuse efficiency models proven by
others, but this assumes that everyone uses the same assumptions.

These assumptions are in our RAM model (see Lecture 1.2). It enables
very precise calculations. It describes a simple sequential
``machine'', yet with good realism and enables reasoning about both
the correctness and resource consumption of programs at the level of
machine-instruction. The downside is that our reasoning directly
depends on this RAM model. How to guarantee that everyone uses the
same RAM?

\marginnote{
  \includestandalone[width=\marginparwidth]{images/averages2.tikz}
  \captionof{figure}{Efficiency of average algorithms, assuming a RAM
    where multiplication and division by $n$ cost $n$.}
  \label{plot:averages2}
}

Contrast for example an augmented-RAM, which has dedicated
instructions for all arithmetic operations, with a simpler RAM with
only addition and subtraction (see Lecture 1.2). Because the later can
only add, any program must ``unfold'' every multiplication into a
sequence of additions. The cost of multiplication and division by $n$
is not 1 anymore, but $n$! As shown on Figure~\ref{plot:averages2},
the classical average would thus need $6n+4$ while the online average,
which performs many divisions, would need $\frac{n^2+9n+6}{2}$
operations!

We loose in generality what we gain in precision. A more realistic
machine model enables more precise estimations, but these estimation
are only valid for that machine. Our claims about efficiency thus
always assume a specific machine and a cost model. If we change these
assumptions we compromises our conclusions. There is no way out here,
the reasoning we make about a program depends on the underlying model
of computation.

\begin{takeaway}
  Comparing the efficiency of algorithms is only meaningful when the
  efficiency models assumes the same \emph{model of computation}.
\end{takeaway}

To maximize ``generality'', we strip away the details of our
efficiency models and we will focus on trends, using \emph{asymptotic
  analysis}. The strategy is to:
\begin{itemize}
\item \emph{look at large inputs} because algorithms seldom suffer
  from small input sizes. For small inputs size, differences of dozen
  of instructions is about a few nanoseconds at most. But for very
  large values, the differences may be about centuries.
\item \emph{make qualitative statements} that do not focus on precise
  numerical values but capture the ``way'' the resource consumption
  ``grows'' as the size of input increases.
\end{itemize}


\section{Asymptotic Analysis}
Asymptotic analysis does not directly relate to Computer Science. It
is the tool we borrow from Mathematics to classify the efficiencies of
our algorithms. Intuitively, we use asymptotic analysis to identify
the overall shape of a function, as we would do with everyday life
objects, when we state that this has a square shape or a round shape,
etc. The functions we will manipulate are the efficiency models.

The idea is to find some sort of ``bounding box'' around a complicated
function of interest, say $f(n)$, using families of functions. We will
the ``big-Oh'' notation to describe these bounds:

\begin{itemize}
\item Upper bounds (Big-O) are families of functions that are always
greater than $f$ given a constant factor.
\item Lower bounds (Big-$\Omega$) are families of functions that are
  always lesser $f$ given a constant factor
\item Approximations (Big-$\Theta$) are families of functions that
  resemble $f$ given constant factors.
\end{itemize}

\subsection{Upper Bounds using Big-O}

\marginnote{
  \includestandalone[width=\marginparwidth]{images/big-O.tikz}
  \captionof{figure}{$f \in O(g)$: $g$ is an upper bound for the function $f$}
  \label{fig:bigO}
}

Upper bounds are functions that are always greater for large
inputs. If a function $f$ admits an upper bound $g$, we can think of
it as $f \leq g$. Figure~\ref{fig:bigO} illustrates this idea.

Formally, a function $f(n)$ admits another function $g(n)$ as an upper
bound if we can find two constants $c$ and $n_0$ such as the product
$c \cdot g(n)$ is greater than or equals to $f(n)$ for every $n$
greater than $n_0$. That is:
\begin{equation*}
  \begin{split}
  f \in O (g) & \iff \\
  & \exists \: c \in \mathbb{R}, \; \\
  & \qquad \exists \: n_0 \in \mathbb{N}, \;  \\
  & \qquad \qquad \forall \: n \geq n_0,\; f(n) \leq c \cdot g(n) 
  \end{split}
\end{equation*}

\subsection{Lower Bounds using Big-$\Omega$}

A lower bound is the counter part of an upper bound: This
bound is a function that is ``lesser'' than the function of
interest. Visually, the lower is ``below'' as shown in
Figure~\ref{fig:bigOmega}. I like to think of a lower bound $g(n)$ as
a functinon such as $g(n) \leq f(n)$.

\marginnote{
  \includestandalone[width=\marginparwidth]{images/big-Omega.tikz}
  \captionof{figure}{$f \in \Omega(g)$: $g$ is a lower bound for a
    function $f$}
  \label{fig:bigOmega}
}

The definition mirrors the one of the upper bound. Provided a function
$f(n)$, we say that $f$ admits at lower bound $g(n)$, if there exists
two constants $c$ and $n_0$ such as the product $c \cdot g(n)$ remains
lesser than or equal to $f(n)$ for each $n$ greater than or equal to
$n_0$. We denote lower bounds with the Greek letter Omega
(big-$\Omega$) as follows:

\begin{equation*}
  \begin{split}
  f \in \Omega (g) & \iff \\
  & \exists \: c \in \mathbb{R}, \; \\
  & \qquad \exists \: n_0 \in \mathbb{N}, \;  \\
  & \qquad \qquad \forall \: n \geq n_0,\; c \cdot g(n) \leq f(n) 
  \end{split}
\end{equation*}

\subsection{Approximations using Big-$\Theta$}

Finally we can also search for a single function that approximates our
model. This is the big-Theta notation, which finds both an upper and a
lower bound at the same time. I like to think of this
$ g(n) \approx f(n)$

\marginnote{
  \includestandalone[width=\marginparwidth]{images/big-Theta.tikz}
  \captionof{figure}{$f \in \Theta(g)$: $g$ is an approximation of the
    function $f$}
  \label{fig:bigTheta}
}

Provided a function $f(n)$, we say that $f$ is the range of $g(n)$, if
there exists three constants $c_1$, $c_2$ and $n_0$ such as the
product $c_2 \cdot g(n)$ remains below $f(n)$ and the product
$c_1 \cdot g(n)$ remains above $f(x)$ for each $n$ greater than or
equal to $n_0$. We denote ranges with the Greek letter Theta
(big-$\Theta$), which we formally define as follows:

\begin{equation*}
  \begin{split}
  f \in \Theta(g) & \iff \\
  & \exists \: (c_1, c_2) \in \mathbb{R}^2, \\
  & \qquad \exists \: n_0 \in \mathbb{N}, \\
  & \qquad \qquad \forall \: n \geq n_0, \\
  & \qquad \qquad \qquad c_2 \cdot g(n) \leq f(n) \leq c_1 \cdot g(n)
  \end{split}
\end{equation*}

\subsection{Other Types of Bounds}

There are two additional classes of bounds which are less commonly
used, but I add them here for the sake of completeness. They are the
\emph{little-o} and \emph{little-$\omega$}.

\paragraph{Little-o} Little-o also represents a family of functions
that accept an upper bound, but the definition is stricter. Little-o
demands that the product $c \cdot g(x)$ be \emph{strictly greater
  than} $f$, and \emph{for all} possible values of $c$.  Formally, we
defined \emph{little-o} as follows:
\begin{equation*}
  \begin{split}
    f \in o(g) & \iff \\
    & \forall \: c \in \mathbb{R}^+, \\
    & \qquad \exists \: n_0 \in \mathbb{N}, \\
    & \qquad \qquad \forall \: n \geq n_0, \; c \cdot g(n) > f(n)
  \end{split}
\end{equation*}
  
Another way to look at the little-o approximation are those functions
that are upper-bounds but not range. Formally
$f\in o(g) \iff f \in O(g) \land f \not\in \Theta(g)$.
          
\paragraph{Little-$\omega$} Just as big-Omega is the counter part of
big-O, \emph{little-$\omega$} is the counter-part of
little-o. Little-$\omega$ denotes the class of functions that accepts
$g(n)$ as a lower bound such that \emph{for every possible constant
  $c$}, there exist a constant $c$, such that the product
$c \cdot g(n)$ be \emph{strictly lower} than $f(x)$ for all values of
n greater than $n_0$. Formally, we define \emph{little-$\omega$} as
follows:
\begin{equation*}
  \begin{split}
    f \in \omega(g) & \iff \\
    & \forall \: c \in \mathbb{R}^+, \\
    & \qquad \exists \: n_0 \in \mathbb{N}, \\
    & \qquad \qquad \forall \: n \geq n_0, \;  c \cdot g(n) < f(n)
  \end{split}
\end{equation*}

Both little-o and little-$\omega$ place stronger constraints on the
bounds and therefore lie further away from the model they
describe. The are so called "loose" bounds.

\paragraph{Tights bounds} A bound is said to be ``tight'', when there is
no better ``closer'' for a given function (see Chap. 3 in
\cite{preiss2008}). Note that the expression "tight bounds"
sometimes refer big-$\Theta$.  Intuitively, the tightest bound is the
"minimum" bound, that is, the bound that is smaller than all the
others. Formally, given two functions $f$ and $g$, such that
$f \in O(g)$, would be the "tightest" bound if and only if:
$\forall h, \, f \in O(h) \implies g \in O(h)$.

\section{Orders of Growth}

\subsection{Classification}

As for algorithm efficiency we will use asymptotic analysis with
pre-existing growths, as listed in Table~\ref{tab:growths} (and shown
on Figure~\ref{plot:growths}). These growths orders capture how the
efficiency grows with the input size. A constant growth indicates that
the efficiency does not depends on the input size. By convention, an
efficient algorithm is an algorithm whose approximation at most
linear. Anything that grows faster than a linear relationship is seen
as inefficient. We will meet many problems for which the best known
algorithms are still not ``efficient''.

\begin{takeaway}
  We use \emph{asymptotic analysis} to simplify the models we obtain
  from \emph{algorithm analysis}. Any kind of bound can possible
  describe any kind of scenario (best, worst or average).
\end{takeaway}

\begin{table}[htbp]
  \marginnote{
    \includestandalone[width=\marginparwidth]{images/growths.tikz}
    \captionof{figure}{Common growth orders}
    \label{plot:growths}
  }
  \begin{center}
    \begin{tabular}{llrrr}
      \toprule
      Name        & Formula       & \multicolumn{3}{c}{Efficiency ($k=2$)}                   \\
      \cmidrule(l){3-5}
                  &               & $n=10$             & $n= 100$               & Growth     \\
      \midrule
      Constant    & $k$           & 2                  & 2                      & $1$        \\
      Logarithmic & $\log_k n$    & 3.32               & 6                      & $2$        \\
      $k$-th root & $\sqrt[k]{n}$ & 3.16               & 10                     & $3$        \\
      Linear      & $n$           & 10                 & 100                    & $10$       \\
      \hdashline
      Log-linear  & $n \log_k n$  & 33                 & 664                    & $20$       \\
      Polynomial  & $n^k$         & 100                & \numprint{10000}       & $100$      \\
      Exponential & $k^n$         & 1024               & $1.26 \times 10^{30}$  & $10^{26}$  \\
      Factorial   & $n!$          & \numprint{3628800} & $9.33 \times 10^{157}$ & $10^{151}$ \\
      \bottomrule 
    \end{tabular}
  \end{center}
  \caption{Main growth orders used in Computer Science}
  \label{tab:growths}
\end{table}
  
  Some problems are \emph{intractable} because the only algorithm
  known to solve have such low efficiency than solving any realistic
  instance would take forever.


\subsection{In Practice}

Computing bounds is more of an academic exercise but I found it useful
to know how to do. There are three steps:

\begin{enumerate}
\item Find the efficiency model. Refer to Lecture 2.1 2.2 if counting
  the number of instructions executed or the number of memory cells
  used is unclear. Consider for example the expression we got for the
  online average running on a RAM with only addition and subtraction
  (see Section~\ref{sec:generality}). 
  \begin{equation}
    f(n) = \frac{n^2+9n+6}{2}
  \end{equation}
\item Identify the ``bound'' $g(n)$. To this end, simplify the formula
  by keeping only the most significant term (the highest-order term)
  and removing the constant factor. On the previous example, that
  gives:
  \begin{align*}
    f(n) & = \frac{n^2+9n+6}{2} \\
    & \leadsto \frac{n^2}{2} \tag{highest order term} \\
    & \leadsto n^2 \tag{constant factors}
  \end{align*}
\item \marginnote{
    \includestandalone[width=\marginparwidth]{images/bounds.tikz}
    \captionof{figure}{Visualizing the lower and upper bounds}
    \label{plot:bounds}.
  }
  Find the constants $c$ and $n_0$ and check that the relationship
  you are working with (O, $\Omega$ or $\Theta$) holds for all inputs
  size greater than $n_0$. We make a guess at the constants $c$ and
  check it the inequalities holds. Say for example we want to
  establish that $f \in \Theta(n^2)$. We try with $c_1 = 1$ and we
  check that:
  \begin{align*}
    f(n) & \leq c \cdot g(n) \\
    f(n) & \leq 1 \cdot n^2 \\
    n^2+9n+6 & \leq 2n^2 \\
    0 & \leq n^2 - 9n -6 \\
    n & \geq \frac{1}{2} \cdot \left(9 + \sqrt{105}\right) \\
    n & \geq 10
  \end{align*}
  That gives us a possible values for $n_0$. We proceed similarly for
  $c_2$. A possible guess could be $c_2=\frac{1}{2}$. That gives us
  another value for $n \geq -\frac{2}{3}$. Theta holds on the interval
  where $g$ is both an upper bound and a lower bound, that is when
  $n\geq 10$. As shown on Figure~\ref{plot:bounds}, $f$ thus admits
  $g(n)=n^2$ both as a lower and upper bound for $n \geq 10$, so we
  have established that $f \in \Theta(n^2)$.
\end{enumerate}

\subsection{Pitfalls}

In my experience, this notation is very useful, as it conveys a lot of
information. Say I want to sort huge collections of books, I can
quickly browse through existing sorting algorithms and see that common
have a log-linear time-efficiency, while naive ones are the most
time-efficient.

\paragraph{Same Computation Model?} As we saw in
Section~\ref{sec:generality} we must remember that these bounds are
most often computed for a RAM. So if our implementation relies on a
different model, say its uses multiple thread or processes, then the
bound is irrelevant.

\paragraph{Same Scenario?} Most often, bounds are computed for the
worst case. But is this what we need in practice. In many cases, the
worst cases may not be representative, because it is a very rare cases
or acceptable. The average case may be more relevant then.

\paragraph{Same Growth Order?} Sometimes, two alternatives fall in the
same family while they may be very different. Consider for example
$f_1(n) = 1000n$ and $f_2(n)=10n$. They both are in the order of
linear functions, but $f_1$ is 100 time faster. That's a huge speed up
in practice.

\paragraph{Expected Input} The documented bounds assume very large and
``random'' inputs. But this may not be the cases in practice. One may
be sorting arrays that are not completely randomized but only slightly
and then some so-called inefficient algorithm actually perform the
best. The same applies for input size the bound say nothing about
small input sizes (where $n < n_0$).

\section*{Conclusions}

Now we know how to identify and compare algorithms' efficiency. We do
that by identifying the underling growth order. That tells us directly
whether or not an algorithm will ``scale'' to large inputs and deliver
its results in a reasonable amount of time.

This closes the foundations of our courses. We have covered quite some
ground: We started with general definitions about computation and
algorithms, them we looked at RAM, which enables reasoning about
correctness and efficiency. You now much enough ``theory'' and will
now start looking at various data structure and algorithms to use
them! We start with the array next week!

\bibliographystyle{acm}
\bibliography{references}

\end{document}