#+title: Arrays
#+subtitle: Storing Collections in Contiguous Memory Cells
#+author: Franck Chauvel
#+date: June 4, 2021
#+language: en

#+SETUPFILE: ../templates/style.org


* Introduction

  Let's look at our first data structure, the arrays. We will see what
  is an array and what we can do with them.

  As a start, we will look at three algorithms: How to insert a new
  element in an array, how to find a specific element, and how to
  remove an element from an array. These will be good opportunities to
  practice calculating [[file:orders_of_growth.org][orders of growth.]]

  We will focus on fixed-size arrays for now, but will look dynamic
  array in the [[file:dynamic_arrays.org][next lecture]].

* Arrays

** What Is an "Array"?

  An array is a data structure. It is */a contiguous allocation of
  similar items in memory so that we can access then by index/*. There
  are several important things here that make arrays so special:
   - The items are allocated next to each other, no space in left in
     between. I will refer to the space allocated for an element as a
     /bucket/.
   - The items are similar in size, that is, they all occupy the same
     number of memory cells. In other words, each bucket as the same
     size.
   - The maximum number of items is fixed, and we will refer to this
     as the /capacity/. This is true only for static arrays and we
     will see how to circumvent this in the [[file:dynamic_arrays.org][next lecture on dynamic
     arrays]]. I will also refer to the actual number of items in the
     array as the /length/, that is, the number of buckets in use at a
     given point in time.

  Figure [[fig:allocation]] below portrays an array allocated in memory. Here
  have allocated eight cells, from address 26 to 33, and each item has
  size 1. We are using only the first four buckets (address 26 -- 29),
  and the four other are free.
  
  #+headers: :cmdline --transparent
  #+header: :results graphics file
  #+header: :exports results
  #+headers: :file ../assets/images/arrays.png
  #+begin_src ditaa
        +---base address
        |
        v
        
    25  26  27  28  29  30  31  32  33  34  35  36  37  38  
    +---+---+---+---+---+---+---+---+---+---+---+---+---+---+
    :   | W | X | Y | Z | ? | ? | ? | ? |   :   :   :   :   |
    +---+---+---+---+---+---+---+---+---+---+---+---+---+---+
         <------------->
            length (4)

         <----------------------------->
                  capacity (8)
  #+end_src

  #+caption: Allocating an array in memory: Contiguous buckets of fixed-size.
  #+name: fig:allocation
  #+RESULTS:
  [[file:../assets/images/arrays.png]]

  The trick is that with those properties, I can compute the address
  of any given bucket. I only need to know the memory address of the
  first bucket, something I call the /base/ address. Given that the
  address of the i-th bucket is:

  \[ address(i) = base + i * size(bucket) \]

  This is neat because it means that I don't have to iterate through
  the array until I reach the position I am interested in. Arrays
  offer /random access/ as opposed to /sequential access/. Think of an
  old fashion tape, which one had to "fast forward" to the desired
  track. No need for that with arrays.

  Arrays come with three primitive operations, which I will use to
  manipulate them: create, get, and set.
    - ~create~ new array of a given /capacity/. Each language has its
      own syntax to create an array but the idea remains the same.
    - ~get~ the value stored at a given index.
    - ~set~ a given value at a given index.

  As a quick example, let see how we create an array in C.

    #+name: code:array-definitions
    #+begin_src C -n -r
      #include <stdio.h>

      #define CAPACITY 20

      int length = 5;

      int array[CAPACITY] = {5, 4, 3, 2, 1};

      int show_array() {
        printf("[ ");
        for (int index = 0; index < length ; index++) {
          if (index < length-1) {
            printf("%d, ", array[index]);
          } else {
            printf("%d ]\n", array[index]);
          }
        }
      }
    #+end_src



    #+name: code:array-print
    #+headers: :results output
    #+headers: :exports both
    #+headers: :noweb strip-export
    #+begin_src C -n -r

      int main(int argc, char** argv) {
        show_array();
      }
  #+end_src

  #+RESULTS: code:array-print
  : [5, 4, 3, 2, 1 ]

  In terms of memory consumption, arrays are efficient, because we
  only store data. Often we will need to store their length and
  capacity, but this just two integer value, regardless of the size of
  the array. I can store 3 trillions elements and I still need only
  two extra integers. In addition, array gives us /memory locality/,
  that is buckets are next to each others and this is very good
  caching. A few reads at similar location and the CPU will cache a
  bigger part so that subsequent accesses are much faster. The thing
  is arrays are /cheap and fast/.

  Array can have more than one dimension. One-dimensional array looks
  like a string, 2-dimensional arrays would look like a grid,
  3-dimensional arrays like volume, etc.  Those are simply /nested
  arrays/ that is, arrays of arrays. For example a grid is an array of
  arrays, where each inner array represents one row in the grid. A
  3-dimensional arrays would contain "slides", where each slice is
  thus a grid as just saw. With such a layout, one can access any
  element by modifying the way we compute the address of i-th bucket
  to account for the additional dimensions. For example, for
  2-dimensional grid we got:

  \[ index(row, column) = row * row_capacity + column \]

** Examples

   Arrays are used to store many things from text to pictures to
   vectors and matrices. Let's look at a two examples, text. store cards and
   colored images.

   Starting with text, how could we manipulate text/characters with
   the [[file:rasp_machines.org][random access machines]] we used earlier? There, we can only have
   integers. So if we want to manipulate text, we have to map each
   letters to a specific value add more advance I/O device that could
   render symbols. Here each letter is a single integer, and take
   exactly one memory cell. Creating an array for a text of say 200
   characters is simply reserving 200 memory cells. Once we know the
   address of the first character, we can deduce the address of the
   next 199.

   Let's turn to card-games now. How can we store the hand of a player
   during a Poker game? Each hand has the same number of card, but a
   card is identified by two information, its rank (7, 8, 9, Jack,
   etc.) and its family (heart, club, spade, and diamonds). Here, for
   each player we need an array whose size matches the size of the
   hand, say six cards for example. But each bucket must have a size
   of 2, to hold both the family and rank of the card. Still once we
   know the address of the first card, we can deduce the address of
   all the other.

   Images, now. A raster image is a long list of color points (the
   pixels) arranged in a rectangular grid. Each point defines three
   color channel, red, green and blue as an integer between 0
   and 255. Such images are 2-dimensional arrays of color points,
   where each color points has a size of 3. Once we know the address
   of the top left point, we can compute the address of any other
   given its position in the image (i.e, row and column).


* Search

  /Search/ is an algorithm, which given a value, return the index of
  the bucket where this value lie, or -1 if this value cannot be
  found.

** Algorithm

   The algorithm we use is named the linear search because it
   traverses the array from the first bucket to the last. I don't
   think there is a relationship with the linear growth order.

   Here is how it works. We start looking at the first bucket and we
   check the value it contains. If this the desired value we returned
   the index and we are done. Otherwise, we move to next bucket and
   look there. If the desired value is still not there, we move the
   third bucket, and so on and so forth until we either find the
   desired value or we reach the end of the array, in which case we
   return -1 to indicate that we could not find the desired
   value. Figure [[code:linear-search]] details the C-code associated.

  #+caption: The "linear search" algorithm for arrays
  #+name: code:linear-search
  #+headers: :results output
  #+headers: :exports both
  #+headers: :noweb strip-export
  #+begin_src C -n -r
    <<code:array-definitions>>
    int search(int value) {
      for (int index = 0; index < length ; index++) { (ref:ls-loop)
        if (array[index] == value) { (ref:ls-test)
          return index; (ref:ls-exit)
        }
      }
      return -1; (ref:ls-exit2)
    }

    int main(int argc, char** argv) {
      printf("Found %d at: %d\n", 4, search(4));
      printf("Found %d at: %d\n", 12, search(12));
    }
  #+end_src

  #+RESULTS: code:linear-search
  : Found 4 at: 1
  : Found 12 at: -1

** Cost Model

   Let's find the runtime complexity of this "linear search", shown
   by Listing [[code:linear-search]]. Let's skip the memory analysis and
   focus instead on the runtime since the memory consumption is
   constant: We only use 1 single variable, namely ~index~.

   We don't want to return to the RAM machine and write some assembly
   code. So the first step is to describe a cost model to make
   explicit what we are counting.

   Here, I suggest to count comparisons, that is the number of time
   we execute Line [[(ls-test)]]. This is the only logic operation aside
   of incrementing the index variable (cf. Line [[(ls-loop)]]). We are
   ready to analysis the best, worse and average cases.

** Best Case

   Let's first look at the best case, that the input with which our
   program is the fastest. The earlier we found the desired value,
   the earlier we exit the loop (see Line [[(ls-exit)]]), and the shorter
   is our runtime. What happens is that we check the first bucket
   (with index 0), and we found the value there (see Line
   [[(ls-test)]]). So have made one single comparison. Without any
   algebra, we can compute that our best case of the order of $n$,
   that is $t \in \Theta(n)$. Nice and sweet. Let's continue with the
   worse case.

** Worse Case

   What input makes our algorithm run the longest? It is when we
   must check every single bucket to find the value, in which case we
   make as many comparison as there are buckets. That our worse case
   is of the order of $n$, or formally, $t\in \Theta(n)$. Not however
   that this worse case occurs in two situation: When the desired
   value is in the last bucket, but also when the desired value if
   not in the array. In that case, we must still check every single
   bucket but we found nothing and exit Line [[(ls-exit2)]].

** Average Case

   Finally, let's look at the average case. This is more involved,
   but it's a good opportunities to compute growth orders. Let's
   take it informally first. Intuitively the average efficiency, if
   the sum of efficiency for all possible cases divided by the number
   of cases. So what are the possible cases?  There are $n+1$. The
   desired value, can be in any of the bucket (that's already $n$
   cases), or it is not in the array (that's an extra case). So what
   is the runtime if the desired value is in the first bucket. Well,
   in that case, we do one single comparison, that's the value we are
   looking for and we return the current index. If the desired value
   is in the second bucket, we check the first one, it is not there,
   we check the second, it is there. We have made two
   comparisons. The same holds if the desired value is the last
   bucket: We check all the buckets one by one, and we found the
   value in the last one. That's $n$ comparisons. Finally, if the
   value is not in any bucket, then we have checked all of them in
   vain, and that also $n$ comparison. So if there were 3 items in
   our array, the average complexity would be $t(n) =
   \frac{1+2+3+3}{4} = 2.25$.

   Let's formalize this to get a formula. Let first define a random
   variable B, whose value indicates in which bucket the desired
   value lies.
   - the random variable $B$ ranges from $-1$ to $n-1$. -1 indicates
     that the desired value is not in the array, whereas other values
     indicates the index of the bucket that contains it. That's $n+1$
     values.
   - I assume a uniform probability distribution for the sake of
     generality. I denote by P(B=i) the probability that B=i, that is
     the probability that the desired value lies in the i-th
     bucket. This probability remains constant, regardless of the
     value of $B$, that is $P(B=i) = \frac{1}{n+1}$.

   We can now express the runtime as a function of both the input
   size $n$, and the random variable $B$ as follows:

   \[
   t(n,B) = \begin{cases}
       B+1 &  \text{when } 0 \leq B \leq n-1 \\
       n &  \text{when } B={-1} \\
       \end{cases}
   \]

   With this definition the average runtime is the expected value
   $t(n. B)$, which we can calculate as follow:

   \begin{align*}
     E[t(n,B)] & = \sum_{i=0}^{n-1}{ \big[P(B\!=\!i) \cdot t(n,B) \big]} + \big[ P(B\!=\!{-1}) \cdot t(n,-1) \big] \\
               & = \sum_{i=0}^{n-1}{ \big[ \frac{1}{n+1} \cdot (i+1) \big]}+ \left[ n \cdot \frac{1}{n+1} \right] \\
               & = \left[ \frac{1}{n+1} \cdot \sum_{i=0}^{n-1}{(i+1)} \right] + \frac{n}{n+1} \\
               & = \frac{1}{n+1} \cdot \left[ \sum_{i=0}^{n-1}{i} + \sum_{i=0}^{n-1} 1 \right] + \frac{n}{n+1} \\
               & = \frac{1}{n+1} \cdot \left[ \frac{(n-1)[(n-1)+1]}{2} + n \right] + \frac{n}{n+1} \\
               & = \frac{1}{n+1} \cdot \left[ \frac{n(n-1)}{2} + n \right] + \frac{n}{n+1} \\
               & = \frac{1}{n+1} \cdot \left[ \frac{n(n-1)}{2} + \frac{2n}{2} \right] + \frac{n}{n+1} \\
               & = \frac{1}{n+1} \cdot \frac{n(n-1) + 2n}{2} + \frac{n}{n+1} \\
               & = \left[ \frac{1}{n+1} \cdot \frac{n^2+n}{2} \right] + \frac{n}{n+1} \\
               & = \frac{n^2+n}{2(n+1)} + \frac{n}{n+1} \\
               & = \frac{n^2+n}{2(n+1)} + \frac{2n}{2(n+1)} \\
     E[t(n,B)] & = \frac{n^2 + 3n}{2(n+1)} \\
   \end{align*}

   Quick sanity check before we continue: We see that $E[t(3,B)] =
   2.25$ as we found previously intuitively. Figure [[fig:linear-search]]
   show visually the runtime of the best, average and worse case of
   the linear search.

   #+header: :R-dev-args bg="transparent"
   #+header: :results graphics file
   #+header: :exports results
   #+header: :file ../assets/images/linear_search_runtime.png
   #+begin_src R
   random_distribution <- function(n) {
      weights <- sample(0:100, n+1, replace=TRUE);
      return(weights/sum(weights));
   }

   model <- function(n, B) {
     if (B < n) {
       return(B);
     } else {
       return(n);
     }
   }

   pick_category <- function(probabilities) {
       draw <- runif(1);
       accumulator <- 0;
       for (index in seq(length(probabilities))) {
           accumulator <- accumulator + probabilities[index];
           if (accumulator >= draw) {
              return(index);
           }
       }
       return(length(probabilities));
   }

   random_run <- function(n) {
       distribution <- random_distribution(n);
       category <- pick_category(distribution);
       return(model(n, category));
   }

   random_average_run <- function(n) {
       probabilities <- random_distribution(n);
       return(sum(probabilities * sapply(1:n, function(x) {model(n, x)})));
   }

   sizes <- 1:100;
   sample_count <- 500;
   random_sizes <- sample(sizes, sample_count, replace=TRUE);
   plot(random_sizes, sapply(random_sizes, random_run),
        pch=4,
        col="blue",
        xlab="input size (n)",
        ylab="Number of comparisons");

   points(random_sizes, sapply(random_sizes, random_average_run),
          col="grey",
          pch=1)

   expected <- function(n) { (n^2 + 3*n)/(2*n+2) };
   lines(sizes, sapply(sizes, expected), col="black", lty=1);
   worse_case <- function(n) { n };
   lines(sizes, sapply(sizes, worse_case), col="darkred", lty=2);
   best_case <- function(n) { 1 };
   lines(sizes, sapply(sizes, best_case), col="darkgreen", lty=4);
   legend("topleft",
        inset=0.05,
        cex=0.8,
        box.lty=0,
        legend=c("random runs",
                 "average for random distributions",
                 "average runtime for uniform distribution",
                 "worse case",
                 "best case"),
        lty=c(NA, NA, 1, 2, 4),
        pch=c(4, 1, NA, NA, NA),
        col=c("blue", "grey", "black", "darkred", "darkgreen"))

   #+end_src

   #+caption: Visualization of the average time-complexity of the linear search algorithm, shown in Listing [[code:linear-search]]
   #+name: fig:linear-search
   #+RESULTS:
   [[file:../assets/images/linear_search_runtime.png]]

   Now we have found our formula for the average scenario. Let's find
   an approximate upper bound. So, following the definitions in
   [[file:orders_of_growth.org][orders of growth (Chapter 3)]], we must find a function, $g(n)$, a
   constant $c$, and a constant $k$, such as the product $c \cdot
   g(n) \geq t(n, B)$, for all $n \geq k$. As a first guess, I assume
   that $g(n) = n$ and that $c=2$: Let's see where does that take us.
   \begin{align*}
      c \cdot g(n) & \geq t(n, B) \\
      2n & \geq \frac{n^2 + 3n}{2(n+1)} \\
      4n & \geq \frac{n^2 +3n}{n+1} \\
      4n (n+1) & \geq n^2 +3n \\
      4n^2 + 4n & \geq n^2 +3n \\
      4n^2 - n^2 + 4n - 3n & \geq 0 \\
      3n^2 + n & \geq 0 \\
   \end{align*}

   This second-degree inequality $3n^2 + n \geq 0$ holds
   regardless of $n$, so we can pick $k$ as we please. So we have
   shown that our average time-efficiency mode admits an upper bound
   of linear order: \(t \in O(n), \forall \; k \geq 0\).

   One down, one to go. Let's now turn to the lower bound. Again,
   refer to the definition given in [[file:orders_of_growth.org][the previous lecture]]. To find an
   approximate lower bound, we have to find a function $g(n)$, and
   constant $c$, and a constant $k$, such as the product $ c\cdot
   g(n)$ is lower than or equal to $t(n, B)$ for all $n \geq
   k$. Again, as a first guess, I assume that $g(n) = n$, and that
   $c=\frac{1}{2}$. Let see what we get:

   \begin{align*}
      c \cdot g(n) & \leq t(n, B) \\
      \frac{n}{2} & \leq \frac{n^2 + 3n}{2(n+1)} \\
      n & \leq \frac{n^2 +3n}{n+1} \\
      n (n+1) & \leq n^2 +3n \\
      n^2 + n & \leq n^2 + 3n \\
      0 & \leq n^2 - n^2 + 3n - n   \\
      0 & \leq 2n \\
      0 & \leq n \\
   \end{align*}

   This gives use a value for the constant $k$. So we have shown that
   our runtime model accepts and linear lower bound, that is $t \in
   \Omega(n), \forall \; k \geq 0$.

   We can conclude that our runtime is of the order of $g(n) = n$,
   because for any $k \geq 0$, our models admits both a linear upper
   bound and a linear lower bound, that is, $t \in \Theta(n)$.

* Insertion

  Seldom are arrays made once and for all: We often need to insert and
  delete items. Let's first see now how to insert an element.

** At the end

  To simplify, let's consider first that we insert the given element
  at the end of the array, provided there is still room for it. If the
  length equals the capacity, we return an error to indicate that the
  insertion failed.

  #+name: code:back_insertion
  #+headers: :noweb strip-export
  #+headers: :results output
  #+headers: :exports both
  #+begin_src C -n -r
     <<code:array-definitions>>
     int insert(value) {
        if (length >= CAPACITY) return -1;
        array[length] = value;
        length += 1; (ref:push-increment)
        return length;
     }

     int main() {
        insert(9);
        show_array();
     }
  #+end_src

  #+RESULTS: code:back_insertion
  : [ 5, 4, 3, 2, 1, 9 ]

  Listing [[code:back_insertion]] shows how to insert a value at the end
  of the array. We first check if there is still some free buckets
  (cf. Line [[(push:test))]). If there is no, we just return ~-1~. If
  there are some free bucket, we place the value in there and we
  increment the length of the array (cf. Line [[(push-increment))]).

  This is nice because, if we look at the time complexity, we see that
  it does not depends on the length of the array. We do not traverse
  the array: We know it's current length and we directly append the
  new value at the back. This insertion at the end as a constant
  complexity ($\Theta(1)$).

** At an Arbitrary Position

  This all good, but in the real life, we may need to insert a value
  anywhere in the array, and not only at the end. Listing
  [[code:array-insertion]] shows the algorithm to insert a value anywhere
  in an array.


  #+headers: :cmdline --transparent
  #+header: :results graphics file
  #+header: :exports results
  #+headers: :file ../assets/images/array-insertion.png
  #+begin_src ditaa
             (3) "A"   
                  :   
                  |   
    25  26  27  28v 29  30  31  32  33  34  35  36  37  38  
    +---+---+---+---+---+---+---+---+---+---+---+---+---+---+
    :   | W | X | Y | Y | Z | ? | ? | ? |   :   :   :   :   |
    +---+---+---+---+---+---+---+---+---+---+---+---+---+---+
                   | ^ | ^
                   : | : |
                   +-+ +-+
                   (2) (1)
  #+end_src

  #+caption: Inserting "A" in the 3rd bucket, and shifting the other element to the right.
  #+name: fig:insertion-array
  #+RESULTS:
  [[file:../assets/images/array-insertion.png]]


  Figure [[fig:insertion-array]] illustrates how to insert an "A" into the
  third bucket of the array [W, X, Y, Z]. We first need to make some
  room for "A", by shifting the content the 3rd and 4th bucket (i.e.,
  Y and Z) by one bucket to the right. We then get [W, X, Y, Y, Z] and
  we can now insert the value A in the 3 bucket without loosing
  Y. Listing [[code:array-insertion]] below details how to that in C.
 
  #+caption: Insertion at an arbitrary position
  #+name: code:array-insertion
  #+headers: :noweb strip-export
  #+headers: :results output
  #+headers: :exports both
  #+begin_src C -n -r
    <<code:array-definitions>>
    int insert (int value, int position) {
      if (length >= CAPACITY) return -1;
      length += 1; (ref:insert-increment)
      for (int index=length-1; index>=position ; index--) { (ref:insert-loop)
          array[index+1] = array[index]; (ref:insert-shift)
      }
      array[position] = value; (ref:insert-value)
    }

     int main() {
       show_array();
        insert(9, 3);
        show_array();
     }
  #+end_src

  #+RESULTS: code:array-insertion
  : [ 5, 4, 3, 9, 2, 1 ]

  We first shift all the elements beyond the bucket where we have to
  insert the new value. To do that we iterate from the end (cf. Line
  [[(insert-loop)]]) and copy each bucket into the next (cf. Line
  [[(insert-shift)]]). Then we can proceed and insert the value in the
  desired bucket (cf. Line [[(insert-value)]]).

  Let's look quickly at efficiency now. As for the linear search (see
  Section [[*Search]]), the space-efficiency is constant, so there is not
  much more to say here. As for the runtime-efficiency, inserting at
  the end takes a constant time (see [[*At the end]]), whereas inserting
  in front is the worst case. Eventually, the average case is of
  linear order.

* Deletion

  Let's finally look at how we can delete something in an
  array. Deletion resembles insertion, but instead of shifting to
  right, we shift to the left in order to shrink the array.

  Figure [[fig:deletion-array]] illustrates this process. Starting from
  the end, we copy each bucket into the previous until we reach the
  desired bucket. Note there is no need to explicitly clear the last
  cell (which is duplicate), because as we have updated the ~length~,
  subsequent operation will never read this "left-over" value.
  
  #+headers: :cmdline --transparent
  #+header: :results graphics file
  #+header: :exports results
  #+headers: :file ../assets/images/array-deletion.png
  #+begin_src ditaa
          delete "A"   
                  :   
                  |   
    25  26  27  28v 29  30  31  32  33  34  35  36  37  38  
    +---+---+---+---+---+---+---+---+---+---+---+---+---+---+
    :   | W | X | Y | Z | Z | ? | ? | ? |   :   :   :   :   |
    +---+---+---+---+---+---+---+---+---+---+---+---+---+---+
                   ^ | ^ |
                   : | : |
                   +-+ +-+
                   (1) (2)
        
         <-------------->
            new length
  #+end_src

  #+caption: Deleting "A" in the 3rd bucket, and shifting the other element to the left.
  #+name: fig:deletion-array
  #+RESULTS:
  [[file:../assets/images/array-deletion.png]]

  Listing [[code:array-deletion]] below details the C function to delete
  an element at a given position. We first reduce the length of the
  array (cf. Line [[(delete-shrink)]]). Then, we shift back element that
  are beyond the given index by one position on the left. We don't
  need to clear or erase the last element, because we have decremented
  the length of the array (i.e., later traversal will not reach this
  value).

  #+caption: Deletion at an arbitrary position
  #+name: code:array-deletion
  #+headers: :noweb strip-export
  #+headers: :results output
  #+headers: :exports both
  #+begin_src C -n -r
    <<code:array-definitions>>
    int delete (int position) {
      if (length >= CAPACITY) return -1;
      int deleted = array[position];
      length -= 1; (ref:delete-shrink)
      for (int index=position; index<length ; index++) { (ref:delete-loop)
          array[index] = array[index+1]; (ref:delete-shift)
      }
      return deleted;
    }

     int main() {
       show_array();
       delete(3);
       show_array();
     }
  #+end_src

  #+RESULTS: code:array-deletion
  : [ 5, 4, 3, 2, 1 ]
  : [ 5, 4, 3, 1 ]

* Summary

  Now we know what an array is and how we can use it. We can use them
  to efficiently store collection of objects. 
  We have also seen our first three algorithms, namely the /linear
  search/ to find which bucket contains a given value. We also looked
  at how we can insert and delete value at specific indices.

  We've seen, not without effort, how one can calculate the
  runtime-efficiency of these algorithms, which Table X summarizes:

  #+caption: Summary of runtime-efficiency of array algorithms
  #+name: table:array-summary
  | Operation             | Best        | Average     | Worse       |
  |-----------------------+-------------+-------------+-------------|
  | value = get(index)    | $\Theta(1)$ | $\Theta(1)$ | $\Theta(1)$ |
  | index = search(value) | $\Theta(1)$ | $\Theta(n)$ | $\Theta(n)$ |
  | insert(value, index)  | $\Theta(1)$ | $\Theta(n)$ | $\Theta(n)$ |
  | value = delete(index) | $\Theta(1)$ | $\Theta(n)$ | $\Theta(n)$ |

  Feel free to also check out [[file:arrays_lab.org][the related exercises]]. 
  So far, the main limitation is the fixed capacity, which forces us
  to over-allocated when we create an array.

  
  

* Suggested Readings
  
     - [[https://en.wikipedia.org/wiki/Array_data_type][Wikipedia: Array as a data type]]
     - [[https://en.wikipedia.org/wiki/Array_data_structure][Wikipedia: Array as a data structure]]
